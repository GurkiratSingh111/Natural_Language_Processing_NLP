{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eefee55-e815-4075-8ba7-cf6e31b837d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <h3>NOTE</h3>\n",
    "    <p>Before you submit this assignment, <strong>make sure everything runs as expected</strong>:</p>\n",
    "    <ol>\n",
    "        <li><strong>restart the kernel</strong> (in the menubar, select <strong>Kernel → Restart</strong>)\n",
    "        <li><strong>run all cells</strong> (in the menubar, select <strong>Cell → Run All</strong>)</li>\n",
    "    </ol>\n",
    "    <p>Make sure to complete every cell that states \"<strong><TT>YOUR CODE IN THIS CELL</TT></strong>\".</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105bf91-4c5b-4a2c-8d5d-a18980c36c65",
   "metadata": {},
   "source": [
    "# Assignment #2\n",
    "\n",
    "<font color=red>**Due**:<font color=black> Sat Mar 4 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304db598-86a5-47c4-b4a8-989a5379a9fe",
   "metadata": {},
   "source": [
    "<font color=red>**NOTE:**<font color=black> These questions are intended to get your feet wet in the spectrum and **breadth** of open research problems in NLP. They are *not* intended for you to explore a problem in any **depth**.\n",
    "\n",
    "For the coding questions, aim to have a simple, working prototype that you could revisit at a later time if you wanted to improve its performance.\n",
    "    \n",
    "A \"best practice\" is to work these out *by hand* first to know what is reasonable for your code to be able to do (knowing it can be done by a human). An example is identifying whether the word `run` is a verb or not. Can you determine whther `run` is a verb?\n",
    "    \n",
    "Note that some of the tasks in this assignment could be considered a Master's Thesis (if not a PhD thesis)! Do not get frustrated or overwhelmed by the difficulty of each problem.\n",
    "    \n",
    "Feel free to work with others but make sure you are submitting your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84fcac-e085-4239-a956-a373e0e16766",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=blue>\n",
    "    \n",
    "#### OBJECTIVES\n",
    "    \n",
    "<font color=black>\n",
    "\n",
    "* become familiar with the `JupyterLab Notebook` format for the assignments\n",
    "* embrace failure\n",
    "* learn from failure, don't just give up when something doesn't work\n",
    "* get your feet wet in seemingly simple text processing problems\n",
    "* become familiar with `NLTK`\n",
    "* encounter a variety of open problems in natural language processing *prior* to studying them formally (in order to motivate their formal study)\n",
    "* practice the workflow that will be used throughout this course: gather data, implement code, evaluate code, discuss\n",
    "* emphasize the **discussion** and **reflection** aspect of computing science/research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f9def-bb27-40d9-bc83-d1cfcb9d0a67",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<font color=darkred>\n",
    "    \n",
    "# Preparation: NLTK Natural Language Toolkit\n",
    "\n",
    "<font color=black>\n",
    "\n",
    "**NLTK** stands for **Natural Language Toolkit**. It is written in **Python** and intended to be fairly easy to pick up.\n",
    "    \n",
    "The **NLTK book** is found here: https://www.nltk.org/book\n",
    "\n",
    "In order to complete this assignment you must complete Chapters #1, #2, and #3. It is recommended you not just read through the examples but create a **Jupyter Notebook** where you are typing the various commands as you go.\n",
    "\n",
    "A few of your other assignments will use **NLTK** as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a5cbf-1467-4a7a-96c8-1e3aff5cb0e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<font color=darkred>\n",
    "    \n",
    "# Question: Tokenizing Text\n",
    "\n",
    "<font color=black>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84c771-5db9-435a-89fd-bbdd3cb88663",
   "metadata": {},
   "source": [
    "You will process text saved to the `text_data` variable. The variable is of type `String`.\n",
    "\n",
    "This data is know as a (very small) **dataset**, or sometimes referred to as **corpora**.\n",
    "The etymology of <font color=darkred>**corpora**<font color=black> is it comes from **corpus** or **corpse**, meaning the body of something. So **corpora** refers to a **body of texts** (or collection of texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a9a6dd-9df1-4a54-bd98-d1399a8aa383",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"\n",
    "Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently - they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think\n",
    "that they can change the world, are the ones who do.\n",
    "The quote baove is by Steve Jobs. Mr. Jobs also said: I choose a lazy person to do a hard job. Because a lazy person will find an easy way to do it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62116d-9daf-44bc-babb-9647131d4599",
   "metadata": {},
   "source": [
    "Break the above text into **paragraph tokens** (a list of paragraphs).\\\n",
    "How many paragraphs do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3914e2ee-6c71-43a4-b330-d50861b4737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently - they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think\n",
      "that they can change the world, are the ones who do\n",
      "\n",
      "\n",
      "The quote baove is by Steve Jobs. Mr. Jobs also said: I choose a lazy person to do a hard job. Because a lazy person will find an easy way to do it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Number of paragraph tokens: 2\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE IN THIS CELL\n",
    "\n",
    "paragraphs = text_data.split(\".\\n\")\n",
    "count=0\n",
    "for x in paragraphs:\n",
    "    if(x.strip() != '' and len(x)!= 1): \n",
    "        count+=1\n",
    "#Printing the paragraphs\n",
    "for x in paragraphs:\n",
    "    print(x)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"Number of paragraph tokens:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfcbef-a78c-480d-b817-4340b3395916",
   "metadata": {},
   "source": [
    "Break the above text into **sentence tokens**.\\\n",
    "How many sentences do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17d5cc1-5f87-4bd1-8a99-afa9b1a6b3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes\n",
      "\n",
      "\n",
      " The ones who see things differently - they’re not fond of rules\n",
      "\n",
      "\n",
      " You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things\n",
      "\n",
      "\n",
      " They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think\n",
      "that they can change the world, are the ones who do\n",
      "\n",
      "\n",
      "\n",
      "The quote baove is by Steve Jobs\n",
      "\n",
      "\n",
      " Mr\n",
      "\n",
      "\n",
      " Jobs also said: I choose a lazy person to do a hard job\n",
      "\n",
      "\n",
      " Because a lazy person will find an easy way to do it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Number of sentence tokens:  8\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE IN THIS CELL\n",
    "\n",
    "#Using Regex Library\n",
    "import re \n",
    "count_of_sentences = 0\n",
    "sentences = re.split('[?.!;]', text_data)\n",
    "\n",
    "for x in sentences:\n",
    "    if(x.strip() != ''):\n",
    "        count_of_sentences+=1\n",
    "\n",
    "for x in sentences:\n",
    "    print(x)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Number of sentence tokens: \", count_of_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f31d3-9608-4100-9d19-ae7a8aa97def",
   "metadata": {},
   "source": [
    "Break the above text into **word tokens**.\\\n",
    "How many words do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5445676d-2936-4818-83a2-1b4f5f9b16a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s\n",
      "to\n",
      "the\n",
      "crazy\n",
      "ones\n",
      "the\n",
      "misfits\n",
      "the\n",
      "rebels\n",
      "the\n",
      "troublemakers\n",
      "the\n",
      "round\n",
      "pegs\n",
      "in\n",
      "the\n",
      "square\n",
      "holes\n",
      "The\n",
      "ones\n",
      "who\n",
      "see\n",
      "things\n",
      "differently\n",
      "they’re\n",
      "not\n",
      "fond\n",
      "of\n",
      "rules\n",
      "You\n",
      "can\n",
      "quote\n",
      "them\n",
      "disagree\n",
      "with\n",
      "them\n",
      "glorify\n",
      "or\n",
      "vilify\n",
      "them\n",
      "but\n",
      "the\n",
      "only\n",
      "thing\n",
      "you\n",
      "can’t\n",
      "do\n",
      "is\n",
      "ignore\n",
      "them\n",
      "because\n",
      "they\n",
      "change\n",
      "things\n",
      "They\n",
      "push\n",
      "the\n",
      "human\n",
      "race\n",
      "forward\n",
      "and\n",
      "while\n",
      "some\n",
      "may\n",
      "see\n",
      "them\n",
      "as\n",
      "the\n",
      "crazy\n",
      "ones\n",
      "we\n",
      "see\n",
      "genius\n",
      "because\n",
      "the\n",
      "ones\n",
      "who\n",
      "are\n",
      "crazy\n",
      "enough\n",
      "to\n",
      "think\n",
      "that\n",
      "they\n",
      "can\n",
      "change\n",
      "the\n",
      "world\n",
      "are\n",
      "the\n",
      "ones\n",
      "who\n",
      "do\n",
      "The\n",
      "quote\n",
      "baove\n",
      "is\n",
      "by\n",
      "Steve\n",
      "Jobs\n",
      "Mr\n",
      "Jobs\n",
      "also\n",
      "said\n",
      "I\n",
      "choose\n",
      "a\n",
      "lazy\n",
      "person\n",
      "to\n",
      "do\n",
      "a\n",
      "hard\n",
      "job\n",
      "Because\n",
      "a\n",
      "lazy\n",
      "person\n",
      "will\n",
      "find\n",
      "an\n",
      "easy\n",
      "way\n",
      "to\n",
      "do\n",
      "it\n",
      "Number of words tokens: 126\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE IN THIS CELL\n",
    "import string\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "new_string = text_data.translate(translator)\n",
    "\n",
    "new_words = new_string.split();\n",
    "for x in new_words:\n",
    "    x = x.strip()\n",
    "for x in new_words:\n",
    "    print(x)\n",
    "\n",
    "print(\"Number of words tokens:\", len(new_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a59c6-1fae-4901-bf4d-c3b18be7f381",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "<font color=darkred>\n",
    "    \n",
    "# Gathering Results: Frequency Distributions\n",
    "    \n",
    "<font color=black>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5c3af-72f4-4311-815c-9c523c7a2c0a",
   "metadata": {},
   "source": [
    "Determine what the most frequent words in the `text_data` variable are. List the top 20 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9db34a-e594-4ba6-8212-788b555a1100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 most frequent words are:\n",
      "the\n",
      "ones\n",
      "them\n",
      "to\n",
      "do\n",
      "crazy\n",
      "who\n",
      "see\n",
      "because\n",
      "they\n",
      "a\n",
      "things\n",
      "you\n",
      "can\n",
      "quote\n",
      "is\n",
      "change\n",
      "are\n",
      "jobs\n",
      "lazy\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE IN THIS CELL\n",
    "import string\n",
    "\n",
    "#Removing the punctuations\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "new_string = text_data.translate(translator)\n",
    "\n",
    "#Split the text_data into a list\n",
    "new_words = new_string.split();\n",
    "\n",
    "#Changing all the words into lower case\n",
    "lowercase_list = [element.lower() for element in new_words]\n",
    "\n",
    "#Creating a dictionary to store the word frequency\n",
    "word_freq = {}\n",
    "\n",
    "for word in lowercase_list:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "        \n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Getting 20 most frequent words\n",
    "most_frequent_20_words = [word[0] for word in sorted_word_freq[:20]]\n",
    "\n",
    "print(\"The top 20 most frequent words are:\")\n",
    "for words in most_frequent_20_words:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52aa57-1309-475e-89c1-ce70a7900f44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<font color=darkred>\n",
    "    \n",
    "# Reflection & Thoughts\n",
    "    \n",
    "<font color=black>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ad57e-647a-48e8-a28b-19005a9ecffd",
   "metadata": {},
   "source": [
    "Discuss the following:\n",
    "* how well did your code do at tokenizing the text into words?\n",
    "* how did you evaluate whether it performed well?\n",
    "* is there anything surprising or unusual in your findings/results?\n",
    "* why did you get the results you did?\n",
    "* what can you do better to improve the performance/results?\n",
    "* what appears to potentially provide the biggest boost in performance if you could have taken care of the shortcoming?\n",
    "* how represent is the dataset of text data in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f8c20-c1ca-470e-965a-cd805381b16f",
   "metadata": {},
   "source": [
    "Reflection and Thoughts: \n",
    "\n",
    "Paragraph Tokens:\n",
    "In the case, we are tokenizing the text into paragraph tokens, we split the text into paragraph(list) at \".\\n\" using .split function and then returning the count of the elements in the list that are non-empty and have more than one character which means we are assuming each paragraph ends with a period followed by a newline character.I evaluated the code by a couple of test cases including the one provided(text_data). The code runs perfectly for the given text_data (test case)and returned 2 as the number of paragraphs in the text_data because all the paragraphs in the text_data ends with period followed by newline character which is usually the case.But it is not always the case. The paragraph can even end with exclamation point(!), question mark(?), semicolon(;) etc.To handle these cases and to improve the performance/results, we need to optimize our program to handle the case where paragraphs end with !,?, ; etc. More sophisticated text processing techniques, such as regular expressions or natural language processing libraries like NLTK will be needed to successfully handle those cases. This could potientially be the biggest boost in the performance. I think if the dataset had few more paragraphs that ended with exclamation point(!), question mark(?), semicolon(;) and other punctuation marks then it would have been a good text data in general.\n",
    "\n",
    "Sentence Tokens: \n",
    "In this case, we are tokenizing the text into sentences, we split the text into sentences using RegEx library to identify sentence-ending punctuations marks like period, exclamation mark, semicolon and question mark.I evaluated the code by a couple of test cases including the one provided(text_data).The code did well in splitting the text into sentences.But there are still some inconsistencies like in the text_data, there are some abbreviation like Mr. which contains period that does not necessarily indicate end of sentence which resulted in our dataset to split into one extra token i.e. 8 sentence tokens instead of 7 sentence tokens. Also, some sentences may not even end with one of the punctuation marks specified in the regular expression, such as sentences that end with a dash or a closing quotation mark. To successfully handle these cases and to improve the performance/results, more advanced natural language processing techniques, such as using a pre-trained model is required. \n",
    "\n",
    "Word Tokens:\n",
    "In this case, we are tokenizing the text into words, we first removed the punctuations from the given test_data so that punctuation marks are not counted as separate words and then split it into word tokens. The program returns 126 word tokens in total(excluding the punctuation marks) which is practically highly accurate model. I evaluate the performance of the code using several test cases. The program might not handle non-ASCII characters properly like words with accents or non-English characters may be split incorrectly or not at all. Also, the code does not remove numbers or other non-alphabetic characters, so they will be treated as separate words. Taking care of these short-coming will provide a big-boost to the performance/results.\n",
    "\n",
    "Frequency Distributions: \n",
    "In this case, we are listing 20 most frequent words in the sample data. We first removed punctuations from sample data( text_data) to avoid punctuations being counted as words, then spliting the entire text into tokens, changing them to lower-case to avoid words like \"The\" and \"the\" being conisdered as different words and inserting them in a dictionary to keep the count of each word. I evaluated the performance by testing the code with several test cases. The code works perfectly for the given test data and returns ['the', 'ones', 'them', 'to', 'do', 'crazy', 'who', 'see', 'because', 'they', 'a', 'things', 'you', 'can', 'quote', 'is', 'change', 'are', 'jobs', 'lazy'] as the 20 most frequent words in the text_data. \n",
    "The possible shortcoming in our code is that it is not optimized for large datasets, as it creates a dictionary to store the word frequency, which could consume a significant amount of memory and it won't be a good/efficient solution for large datasets. If we are able to tackle this problem it would be potiential boost in the performance of our code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8ce5f-177e-41b4-993d-fc4822e25ef2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<font color=#B6321C>\n",
    "    \n",
    "### RUBRIC\n",
    "\n",
    "<font color=black>\n",
    "\n",
    "I will be evaluating this assignment mostly by your attempt to explore and probe the problem (via code or via curiousity). The assignments will build on developing critical thinking and <font color=darkgreen>**asking questions**<font color=black>,  and <font color=darkred>*not coming up with the answers*<font color=black>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552899d-203c-4653-af7c-71b075eb1e14",
   "metadata": {},
   "source": [
    "A breakdown of the marking for this assignment:\n",
    "* <font color=#B6321C>[**15 marks**]<font color=black> for tokenizing the text:\n",
    "    * <font color=#B6321C>[**5 marks**]<font color=black> for tokenizing paragraphs\n",
    "    * <font color=#B6321C>[**5 marks**]<font color=black> for tokenizing sentences\n",
    "    * <font color=#B6321C>[**5 marks**]<font color=black> for tokenizing words\n",
    "* <font color=#B6321C>[**10 marks**]<font color=black> for presenting the frequency distributions of the top 20 most frequent words\n",
    "* <font color=#B6321C>[**25 marks**]<font color=black> for analysis of the experiment, thoughts about the results acquired, ideas for future work or what could possibly improve the current system's performance, general reflective thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d9d73-cf32-463f-b891-e12b6fc0757a",
   "metadata": {},
   "source": [
    "The total of this assignment is <font color=#B6321C>[**50 marks**]<font color=black>. It will be scaled to be out of 10 on **Blackboard**.\\\n",
    "Most students will get close to perfect.\n",
    "    \n",
    "If you need help, feel free to email me any questions and/or post your questions to <font color=#B6321C>**Discord**<font color=black>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a1355-69cc-45e4-adbf-9069506d1b77",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=darkred>\n",
    "    \n",
    "# QUESTION: Creating Rules To Identify Verbs\n",
    "    \n",
    "<font color=black>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d4813-a8a6-4f74-b9cc-459ef1347bc1",
   "metadata": {},
   "source": [
    "Here is a list of **verbs** taken from https://eslgrammar.org/list-of-verbs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6100859-b6c9-4b3d-b9af-699f118f86fb",
   "metadata": {},
   "source": [
    "> Accept\n",
    "Achieve\n",
    "Add\n",
    "Admire\n",
    "Admit\n",
    "Adopt\n",
    "Advise\n",
    "Agree\n",
    "Allow\n",
    "Announce\n",
    "Appreciate\n",
    "Approve\n",
    "Argue\n",
    "Arrive\n",
    "Ask\n",
    "Assist\n",
    "Attack\n",
    "Bake\n",
    "Bathe\n",
    "Be\n",
    "Beat\n",
    "Become\n",
    "Beg\n",
    "Behave\n",
    "Bet\n",
    "Boast\n",
    "Boil\n",
    "Borrow\n",
    "Breathe\n",
    "Bring\n",
    "Build\n",
    "Burn\n",
    "Bury\n",
    "bury\n",
    "Buried\n",
    "Burying\n",
    "Buy\n",
    "Call\n",
    "Catch\n",
    "Challenge\n",
    "Change\n",
    "Cheat\n",
    "Chew\n",
    "Choose\n",
    "Clap\n",
    "Clean\n",
    "Collect\n",
    "Compare\n",
    "Complain\n",
    "Confess\n",
    "Confuse\n",
    "Construct\n",
    "Control\n",
    "Copy\n",
    "Count\n",
    "Create\n",
    "Cry\n",
    "Damage\n",
    "Dance\n",
    "Deliver\n",
    "Destroy\n",
    "Disagree\n",
    "Drag\n",
    "Drive\n",
    "Drop\n",
    "Earn\n",
    "Eat\n",
    "Employ\n",
    "Encourage\n",
    "Enjoy\n",
    "Establish\n",
    "Estimate\n",
    "Exercise\n",
    "Expand\n",
    "Explain\n",
    "Fear\n",
    "Feel\n",
    "Fight\n",
    "Find\n",
    "Fly\n",
    "Forget\n",
    "Forgive\n",
    "Fry\n",
    "Gather\n",
    "Get\n",
    "Give\n",
    "Glow\n",
    "Greet\n",
    "Grow\n",
    "Guess\n",
    "Harass\n",
    "Hate\n",
    "Hear\n",
    "Help\n",
    "Hit\n",
    "Hope\n",
    "Identify\n",
    "Interrupt\n",
    "Introduce\n",
    "Irritate\n",
    "Jump\n",
    "Keep\n",
    "Kick\n",
    "Kiss\n",
    "Laugh\n",
    "Learn\n",
    "Leave\n",
    "Lend\n",
    "Lie\n",
    "Like\n",
    "Listen\n",
    "Lose\n",
    "Love\n",
    "Make\n",
    "Marry\n",
    "Measure\n",
    "Meet\n",
    "Move\n",
    "Murder\n",
    "Obey\n",
    "Offend\n",
    "Offer\n",
    "Open\n",
    "Paint\n",
    "Pay\n",
    "Pick\n",
    "Play\n",
    "Pray\n",
    "Print\n",
    "Pull\n",
    "Punch\n",
    "Punish\n",
    "Purchase\n",
    "Push\n",
    "Quit\n",
    "Race\n",
    "Read\n",
    "Relax\n",
    "Remember\n",
    "Reply\n",
    "Retire\n",
    "Rub\n",
    "See\n",
    "Select\n",
    "Sell\n",
    "Send\n",
    "Sing\n",
    "Snore\n",
    "Stand\n",
    "Stare\n",
    "Start\n",
    "Stink\n",
    "Study\n",
    "Sweep\n",
    "Swim\n",
    "Take\n",
    "Talk\n",
    "Teach\n",
    "Tear\n",
    "Tell\n",
    "Thank\n",
    "Travel\n",
    "Type\n",
    "Understand\n",
    "Use\n",
    "Visit\n",
    "Wait\n",
    "Walk\n",
    "Want\n",
    "Warn\n",
    "Wed\n",
    "Weep\n",
    "Wink\n",
    "Worry\n",
    "Write\n",
    "Yell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b15ce-48b7-4489-a9b7-3788677b5050",
   "metadata": {},
   "source": [
    "The above list of verbs is not intended to be comprehensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0ccdc-e074-4e87-9562-ccbf7072ab04",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <p>Note that you are not expected to use any actual algorithms or existing code/tools to perform this task. You are expected to approach the task naively and begin from scratch.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230408e9-5d71-4dc6-ae84-58b963f5e0ef",
   "metadata": {},
   "source": [
    "#### READING FROM A FILE\n",
    "\n",
    "Copy the above list of verbs into a file called `list_of_verbs.txt`. Save this file at the same location as `Assignment-2.ipynb`, then execute the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8935fb9f-eae1-482b-b227-0fdc03f11b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accept Achieve Add Admire Admit Adopt Advise Agree Allow Announce Appreciate Approve Argue Arrive Ask Assist Attack Bake Bathe Be Beat Become Beg Behave Bet Boast Boil Borrow Breathe Bring Build Burn Bury bury Buried Burying Buy Call Catch Challenge Change Cheat Chew Choose Clap Clean Collect Compare Complain Confess Confuse Construct Control Copy Count Create Cry Damage Dance Deliver Destroy Disagree Drag Drive Drop Earn Eat Employ Encourage Enjoy Establish Estimate Exercise Expand Explain Fear Feel Fight Find Fly Forget Forgive Fry Gather Get Give Glow Greet Grow Guess Harass Hate Hear Help Hit Hope Identify Interrupt Introduce Irritate Jump Keep Kick Kiss Laugh Learn Leave Lend Lie Like Listen Lose Love Make Marry Measure Meet Move Murder Obey Offend Offer Open Paint Pay Pick Play Pray Print Pull Punch Punish Purchase Push Quit Race Read Relax Remember Reply Retire Rub See Select Sell Send Sing Snore Stand Stare Start Stink Study Sweep Swim Take Talk Teach Tear Tell Thank Travel Type Understand Use Visit Wait Walk Want Warn Wed Weep Wink Worry Write Yell']\n"
     ]
    }
   ],
   "source": [
    "def read_list_of_verbs_from_file():\n",
    "    verb_file = open('list_of_verbs.txt')\n",
    "    file_contents = verb_file.read()\n",
    "    verb_file.close()\n",
    "    return file_contents.split(\"\\n\")\n",
    "\n",
    "list_of_verbs = read_list_of_verbs_from_file()\n",
    "\n",
    "print(list_of_verbs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d78955-8e52-4e6e-9599-4344e313e1eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CREATING A SET OF RULES\n",
    "\n",
    "Use **Table 4.2** (from https://www.nltk.org/book/ch01.html) to create some potential rules/characteriztics for identifying whether a word is a **verb**. For example, if the word ends in `ing` then it can be considered a verb. Another simple rule to implement is to see if the word is in our `list_of_verbs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f51d5b-e45f-4af3-99f4-1efa12cd4951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hat is not a verb\n",
      "run is a verb\n",
      "sadness is not a verb\n",
      "crying is a verb\n",
      "chew is a verb\n",
      "kiss is a verb\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def is_word_a_verb(word):\n",
    "        with open('list_of_verbs.txt') as file:\n",
    "            list_of_verbs = file.read().lower()\n",
    "            list_of_verbs = list_of_verbs.split()\n",
    "        if word.lower() in list_of_verbs:\n",
    "            return True\n",
    "        \n",
    "        if word[:-3] in nltk.corpus.words.words() and word.endswith('ing'):\n",
    "            return True\n",
    "        \n",
    "        if  word[:-2] in nltk.corpus.words.words() and word.endswith('es'):\n",
    "            return True\n",
    "        \n",
    "        if word[:-1] in nltk.corpus.words.words() and word.endswith('s'):\n",
    "            return True\n",
    "        \n",
    "        given_word = nltk.pos_tag([word])[0]\n",
    "        if given_word[1].startswith('VB'):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "for x in testing_data:\n",
    "    if(is_word_a_verb(x)== True):\n",
    "        print(f\"{x} is a verb\")\n",
    "    else:\n",
    "        print(f\"{x} is not a verb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56143823-3aec-48b1-ba43-0ab4ac5ce30c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DATA TO TEST PERFORMANCE OF CODE\n",
    "\n",
    "An example of what the test data could look like is the following list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770beb0e-30da-4343-9a27-79788860280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = [\n",
    "    \"hat\",\n",
    "    \"run\",\n",
    "    \"sadness\",\n",
    "    \"crying\",\n",
    "    \"chew\",\n",
    "    \"kiss\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf469b8-a9d0-4e38-a4be-0541e48b3c25",
   "metadata": {},
   "source": [
    "Your code will take one word at a time from the list and determine whether the word was a verb or not. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234b239-2137-44b3-b110-e8d73e2759c1",
   "metadata": {},
   "source": [
    "`is_word_a_verb(\"hat\")`\n",
    "\n",
    "would return `False`, whereas:\n",
    "\n",
    "`is_word_a_verb(\"crying\")`\n",
    "\n",
    "would return `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43e68f-434a-418e-9d5d-e926d702d7e2",
   "metadata": {},
   "source": [
    "Note that the list of words in `testing_data` are unrelated to each other, meaning they are not necessarily from the same sentence (or even the same document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3f11-ba3a-472f-925a-d77eda5b213d",
   "metadata": {},
   "source": [
    "#### EVALUATION\n",
    "\n",
    "Evaluate how well your code performs on the list of words in `testing_data`.\n",
    "\n",
    "Add your own set of words to `testing_data` to further evaluate how robust your code is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb32e58-6827-447e-8e20-da1cdbf70bb3",
   "metadata": {},
   "source": [
    "#### DISCUSSION\n",
    "\n",
    "Briefly discuss the performance of your code (1-2 paragraphs).\n",
    "\n",
    "What types of verbs did the code perform well on? What types of verbs did it not perform well on?\n",
    "\n",
    "Are there any problems with this approach in identifying whether a word is a verb?\n",
    "\n",
    "Are there any issues with the list of verbs in our `list_of_verbs.txt` file?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717bfc5-9199-43d0-814b-9cc2e25d404b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h4>PRO TIP</h4>\n",
    "    <p>The best approach to this question is to work on <strong>one aspect at a time</strong>. Since one aspect is not going to identify all possible verbs, treat each aspect as a <em>step</em> toward a destination where improvements are likely small increments.</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e8fba-6d8a-4484-9631-0603bab6c794",
   "metadata": {},
   "source": [
    "<font color=#B6321C>\n",
    "    \n",
    "### RUBRIC\n",
    "\n",
    "<font color=black>\n",
    "\n",
    "We will be evaluating this section in part by your **ambition** in trying to capture difficult aspects of a verb, **not how well** you encoded the aspects of the verbs you were trying to capture.\\\n",
    "A breakdown of the marking for this task:\n",
    "* <font color=#B6321C>[**5 marks**]<font color=black> basic features gathered (trivial, used one or two builtin `NLTK`/`Python` methods in a simple way)\n",
    "* <font color=#B6321C>[**5 marks**]<font color=black> distinctly different aspects gathered (advanced), corresponding to aspects not exactly trivial to encode (tried using a few builtin `NLTK`/`Python` methods in a complex way)\n",
    "* <font color=#B6321C>[**5 marks**]<font color=black> discussion of results explaining the performance of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14578a70-664a-44e8-b91e-93a3886d35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rules to Identify Verbs:\n",
    "    The program for identifying the verbs performs well on the verbs that are in the list_of_verbs. It correctly identifies \n",
    "    and returns True if the word is present in the list_of_verbs. It did not work perfectly on few cases\n",
    "    where the given word is not in the list of verbs or in the NLTK word corpus, in such cases, it returns False, which may \n",
    "    not always be accurate.\n",
    "    The potiential problem with this approach is that It uses NLTK to check if the word is a verb, \n",
    "    which may not always be accurate or appropriate for all contexts.It also does not account for irregular verbs or the verbs \n",
    "    with unusual spellings. This issue with list_of_verbs.txt is that this file does not all the verbs that exist (which is\n",
    "    practically to possible) so there might be the case that the word we take as argument in the function is_word_a_verb is \n",
    "    not present in the file. Also, accept will be different from Accept because the program would considered both\n",
    "    of them to be different. To tackle this problem we need to first convert the word to lower-case and then conpare. \n",
    "    The code is efficient and detects most of the verb correctly.\n",
    "    \n",
    "                                                                                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a69643-aa9b-49a2-baf8-03dfc9f86ce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<font color=darkred>\n",
    "    \n",
    "# QUESTION: Identifying Proper Names In A Text Document\n",
    "    \n",
    "<font color=black>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c4b07-a3bc-4ec7-8215-7e3d234606fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"\n",
    "Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think\n",
    "that they can change the world, are the ones who do.\n",
    "The quote baove is by Steve Jobs. Mr. Jobs also said: I choose a lazy person to do a hard job. Because a lazy person will find an easy way to do it. Steve started the apple computer company with Steve Wozniak (aka `Woz'). Little known facts\n",
    "Trump did not like Jobs. Jobs did not like Trump.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedcbdf7-b58e-471e-bbd7-f57152aa6394",
   "metadata": {},
   "source": [
    "Similar to the previous question (\"*Creating Rules To Identify Verbs*\"), write code that automatically identifies all words/terms that are **proper names**. Use the sample text provided in `text_data` above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8815d-2278-4abe-b15e-240d17cc8276",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <p>Note that you are not expected to use any actual algorithms or existing code/tools to perform this task. You are expected to approach the task naively and begin from scßratch.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ce925-ce67-4881-96eb-86b71ff5f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "words = nltk.word_tokenize(text_data)\n",
    "tag = nltk.pos_tag(words)\n",
    "entities = nltk.ne_chunk(tag, binary=True)\n",
    "\n",
    "for entity in entities:\n",
    "    if hasattr(entity, 'label') and entity.label() == \"NE\":\n",
    "        print(\" \".join(word for word, tag in entity.leaves()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b560b6-7efe-4e98-afc6-00a282209276",
   "metadata": {},
   "source": [
    "Examples of **proper names** are:\n",
    "* John\n",
    "* Vancouver\n",
    "* Trudeau\n",
    "* Pierre Trudeau\n",
    "* British Columbia, Canada\n",
    "* Rememberance Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834a7e6-874a-4818-8787-da2646f60583",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Performance of the Code:\n",
    "    The program identifies proper names in the text document. It is designed to perform entity recogition(NER) on the text and\n",
    "    extract proper names. The program performs well on recognizing and extracting proper names that are labeled as NE such as \n",
    "    people, organization, and locations.The program does not perform well on other types of proper names, such as brand names,\n",
    "    product names and scientific terms/names.\n",
    "    The code is designed to perform named entity recognition (NER) on the text and extract proper names.It performs well on \n",
    "    recognizing and extracting proper names that are labeled as named entities (NE) in the text, such as names of people, \n",
    "    organizations, and locations.The code may not perform well on recognizing and extracting other types of proper names, \n",
    "    such as brand names, product names, and scientific terms. These types of proper names are not labeled as NE in the text and\n",
    "    may require additional techniques to identify and extract them accurately. \n",
    "    Additionally, the code may also make errors in identifying NEs, such as recognizing common nouns or adjectives as NEs or \n",
    "    missing some NEs that are mentioned in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27039c-07be-48d2-abfa-344154963ffc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DISCUSSION\n",
    "\n",
    "Briefly discuss the performance of your code (1-2 paragraphs).\n",
    "\n",
    "What types of proper names did the code perform well on? What types of proper names did it not perform well on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef6a18-a2d7-4970-a888-4f5f02d49ec8",
   "metadata": {},
   "source": [
    "<font color=#B6321C>\n",
    "    \n",
    "### RUBRIC\n",
    "\n",
    "<font color=black>\n",
    "\n",
    "Again, we will be evaluating this section for your **ambition** in trying to capture different aspects of a proper name, **not how well** you coded the aspects of the proper names.\\\n",
    "A breakdown of the marking for this task:\n",
    "* <font color=#B6321C>[**5 marks**]<font color=black> basic features gathered (trivial, used one or two builtin `NLTK`/`Python` methods in a simple way)\n",
    "* <font color=#B6321C>[**5 marks**]<font color=black> distinctly different aspects gathered (advanced), corresponding to aspects not exactly trivial to encode (tried using a few builtin `NLTK`/`Python` methods in a complex way)\n",
    "* <font color=#B6321C>[**5 marks**]<font color=black> discussion of results explaining the performance of the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
